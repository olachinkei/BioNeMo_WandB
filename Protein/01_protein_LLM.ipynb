{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5accdbca",
   "metadata": {},
   "source": [
    "# Environment Setup\n",
    "Please check the [wandbのreport](https://api.wandb.ai/links/wandb-healthcare/jm7scchv) for details on setting up the environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27703414-26ac-4aab-bb3c-9135fa45a843",
   "metadata": {},
   "source": [
    "## Weights and Biases setup\n",
    "The progress and charts of model training can be visualized through Weights and Biases (wandb). Please refer to the above report for instructions on setting up wandb. Below, you will set up the entity and project, which are the storage locations for wandb. Please enter any values you wish.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45df53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86458480-45f9-497a-82a2-1bd10d1c1d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"WANDB_ENTITY\"]=\"\"\n",
    "os.environ[\"WANDB_PROJECT\"]=\"BioNeMo_protein_LLM_pretraining\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b551d-2e18-4670-90ab-a277dc036bf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Confirmation of GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf18a8-cd3e-4407-a21c-b721f89b5337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ab40b-b288-44f9-8a09-c1ee1679a0cc",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be07928-e27a-4445-a662-90ca671f1285",
   "metadata": {},
   "source": [
    "During the training of ESM-2nv, two datasets are used. Uniref50 is divided into training, validation, and testing. To increase the size and diversity of the data, minibatches from Uniref50 are sampled during training, but each sequence in this batch is replaced with a sequence from the corresponding Uniref90 cluster. For details, refer to \"Language models of protein sequences at the scale of evolution enable accurate structure prediction.\" Uniref90 is used only during training and not for validation or testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f6367-af69-4734-a6e5-b7ea990c82f3",
   "metadata": {},
   "source": [
    "## Get the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf39d54-e09e-47e7-a786-cd9ff7886ddc",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before preprocessing the data, please decompress the sample data using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2979b0-4537-4e7c-b330-c878a928eb8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /workspace/bionemo/examples/tests/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6147e128237998f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip uniref202104_esm2_qc_test200_val200.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab7850e-4d3c-4c47-a682-bc4608c9190d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "with wandb.init(name=\"data_upload\") as run:\n",
    "    artifact = wandb.Artifact(\n",
    "        name=\"uniref202104_esm2_qc_test200_val200\",\n",
    "        type=\"dataset\",\n",
    "        description=\"uniref202104_esm2_qc_test200_val200 from BioNeMo examples\",\n",
    "        metadata={\"path\":\"/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200.zip\"},\n",
    "    )\n",
    "    artifact.add_dir(\"/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200\")\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4b3d46-4600-4ab9-a94c-6a7d4483aa6b",
   "metadata": {},
   "source": [
    "Next, you will need to set the data path before running the code located below to preprocess the data.\n",
    "\n",
    "/workspace/bionemo/examples/protein/esm2nv/pretrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e1846e-7a33-4cc8-9a54-5e53ba1f1ba5",
   "metadata": {},
   "source": [
    "To correctly set the data path, you can modify the YAML configuration file below, or update these parameters as part of the command when preprocessing data or training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23cdf0-f7a0-4766-81d6-65b4bb36c079",
   "metadata": {
    "tags": []
   },
   "source": [
    "/workspace/bionemo/examples/protein/esm2nv/conf/base_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00e4ba-3b8c-4ef4-8525-09a669ec436a",
   "metadata": {},
   "source": [
    "\n",
    "Then, you will preprocess the data using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a8d24e-e044-433a-8dfd-da782e1d4980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /workspace/bionemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b9c64-f3b9-4cfe-aedc-9586f5daa00f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python examples/protein/esm2nv/pretrain.py\\\n",
    " --config-path=conf\\\n",
    " --config-name=pretrain_esm2_650M\\\n",
    " ++do_training=False\\\n",
    " ++exp_manager.wandb_logger_kwargs.name='preproceed_data_upload'\\\n",
    " ++exp_manager.wandb_logger_kwargs.job_type='data_upload'\\\n",
    " ++wandb_artifacts.wandb_use_artifact_path='${oc.env:WANDB_ENTITY}/${oc.env:WANDB_PROJECT}/uniref202104_esm2_qc_test200_val200:v0'\\\n",
    " ++wandb_artifacts.wandb_log_artifact_name='uniref202104_esm2_qc_test200_val200_preprocessed'\\\n",
    " ++model.data.val_size=500\\\n",
    " ++model.data.test_size=100\\\n",
    " ++model.data.uf50_datapath=/uniref50_train_filt.fasta\\\n",
    " ++model.data.uf90_datapath=/ur90_ur50_sampler.fasta\\\n",
    " ++model.data.cluster_mapping_tsv=/mapping.tsv\\\n",
    " ++model.data.dataset_path=/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200/uf50\\\n",
    " ++model.data.uf90.uniref90_path=/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200/uf90\\\n",
    " ++model.data.train.uf50_datapath=/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200/uniref50_train_filt.fasta\\\n",
    " ++model.data.train.uf90_datapath=/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200/ur90_ur50_sampler.fasta\\\n",
    " ++model.data.train.cluster_mapping_tsv=/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200/mapping.tsv\\\n",
    " ++model.data.val.uf50_datapath=/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200/uniref50_train_filt.fasta\\\n",
    " ++model.data.test.uf50_datapath=/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200/uniref50_train_filt.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eda664-e677-4c0c-ab54-d0b7c592d4e0",
   "metadata": {},
   "source": [
    "Parameters starting with -- are passed as command line arguments to pretrain.py. For example, config-path and config-name specify the folder and YAML file name of the configuration file. This path is relative to pretrain.py. conf refers to examples/protein/esm2nv/conf, and pretrain_esm2_650M refers to examples/protein/esm2nv/conf/pretrain_esm2_650M.yaml.\n",
    "\n",
    "Parameters starting with ++ can be set in the YAML file. For instance, in the pretrain_esm2_650M.yaml inherited from base_config.yaml, you can find the following parameters:\n",
    "\n",
    "* `do_training`: Set to False to only perform data preprocessing without training.\n",
    "* `exp_manager.wandb_logger_kwargs.name`: Specifies the run name for wandb.\n",
    "* `exp_manager.wandb_logger_kwargs.job_type`: Specifies the job type for wandb (job type helps organize run information later; it does not affect the implementation).\n",
    "* `wandb_artifacts.wandb_use_artifact_path`: Sets the path for the wandb artifacts of the data being preprocessed.\n",
    "* `wandb_artifacts.wandb_log_artifact_name`: Specifies the name of the wandb artifacts for saving the preprocessed data.\n",
    "* `model.data.val_size and model.data.test_size`: Specify the sizes of the validation and test datasets.\n",
    "* `model.data.uf50_datapath`: Specifies the path to the uniref50 fasta file.\n",
    "* `model.data.uf90_datapath`: Specifies the path to the uniref90 fasta file.\n",
    "* `model.data.cluster_mapping_tsv`: Specifies the path to the file that maps uniref50 clusters to uniref90 sequences.\n",
    "* `model.data.dataset_path`: Specifies the output directory path for the preprocessed uniref50 data, which will include splits for training, validation, and testing.\n",
    "* `model.data.uf90.uniref90_path`: Specifies the output directory path for the preprocessed uniref90 data, which contains only the folder u90_csvs and does not include splits for training/testing/validation as uniref90 is used only for training.\n",
    "* `model.data.train.uf50_datapath`: Specifies the path to the uniref50 fasta file.\n",
    "* `model.data.train.uf90_datapath`: Specifies the path to the uniref90 fasta file.\n",
    "* `model.data.train.cluster_mapping_tsv`: Specifies the path to the file that maps uniref50 clusters to uniref90 sequences.\n",
    "* `model.data.val.uf50_datapat`: Specifies the path to the uniref50 fasta file.\n",
    "* `model.data.test.uf50_datapath`: Specifies the path to the uniref50 fasta file.\n",
    "\n",
    "\n",
    "You can also directly modify the YAML file instead of overriding the arguments through the command line. Once processing is complete, the preprocessed data will be located in /workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200/uf50/uf50/.\n",
    "\n",
    "If you want to use your own data for pretraining, fine-tuning, or inference, specify the path as /workspace/bionemo/mydata/. However, you must align the structure and format of your data with the sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb2226-b289-4740-8c86-cfcbd85e086d",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e97a3-18ac-4d36-ab14-405a78e219fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that the preprocessing of the UniRef50 and UniRef90 subsets is complete, you can begin pretraining the ESM-2nv model. The BioNeMo Framework provides checkpoints for pretrained models such as ESM-1nv, ESM-2nv (650M, 3B, 15B), ProtT5nv, and MegaMolBART. The weights for these models can be downloaded from NVIDIA's NGC. If you already have a model checkpoint, you can also resume pretraining from that checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797cb3a-94af-4110-8e27-e9933713be06",
   "metadata": {},
   "source": [
    "You are now ready to start pretraining the model with the following command, taking into account the parameters described:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862fedc-3c75-4c6a-8f11-bab1fd675dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /workspace/bionemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91872f4d-e3a4-4ff4-891a-4ddc0b656aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187fc427-f5d6-4728-a351-486da008721a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python examples/protein/esm2nv/pretrain.py \\\n",
    " --config-path=conf \\\n",
    " --config-name=pretrain_esm2_650M \\\n",
    " ++do_training=True \\\n",
    " ++do_testing=True \\\n",
    " ++wandb_artifacts.wandb_use_artifact_path='${oc.env:WANDB_ENTITY}/${oc.env:WANDB_PROJECT}/uniref202104_esm2_qc_test200_val200_preprocessed:v0'\\\n",
    " ++model.data.dataset_path=/ \\\n",
    " ++model.data.uf90.uniref90_path=/uf90 \\\n",
    " ++trainer.devices=1 \\\n",
    " ++model.tensor_model_parallel_size=1 \\\n",
    " ++model.micro_batch_size=8 \\\n",
    " ++trainer.max_steps=10 \\\n",
    " ++trainer.val_check_interval=1 \\\n",
    " ++exp_manager.create_wandb_logger=True \\\n",
    " ++exp_manager.checkpoint_callback_params.save_top_k=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e70b74-e025-4793-aa50-9d70afb00637",
   "metadata": {},
   "source": [
    "Explanation of Parameters:\n",
    "* `do_training`: Set to True to train the model, assuming that the data has been preprocessed.\n",
    "* `do_testing`: Set to False to skip testing.\n",
    "* `wandb_artifacts.wandb_use_artifact_path`: Enter the path of the dataset used for training.\n",
    "* `model.data.dataset_path`: Specifies the path to the preprocessed uniref50 data folder that includes training/validation/test splits.\n",
    "* `model.data.uf90.uniref90_path`: Specifies the path to the preprocessed uniref90 data. This folder should contain a separate folder named u90_csvs, which must include files from x000.csv to x049.csv.\n",
    "* `trainer.devices`: Specifies the number of GPUs to use.\n",
    "* `model.tensor_model_parallel_size`: Sets the tensor model parallel size.\n",
    "* `model.micro_batch_size`: Sets the batch size. Increase this as much as possible unless memory errors occur.\n",
    "* `trainer.max_steps`: Specifies the maximum number of training steps. Set to 100 for demonstration purposes. One step equals processing one batch. First, calculate total_batches = total number of samples / batch size. If you want to train for N epochs, set max_steps to N * total_batches.\n",
    "* `trainer.val_check_interval`: Specifies the interval at which to run the validation set.\n",
    "* `exp_manager.create_wandb_logger`: Set to False to disable logging to wandb. If set to True, you must provide a wandb API key.\n",
    "* `exp_manager.checkpoint_callback_params.save_top_k`: Specifies the number of best checkpoints to save.\n",
    "The trained results will be saved in /workspace/bionemo/results/nemo_experiments/.\n",
    "\n",
    "With these settings, you are well-equipped to execute the command and start the pretraining process efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aaa7e0-22ba-44e1-89a8-7cc3a17cb0cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905a97e-5274-4873-85b4-321204c79087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /workspace/bionemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3854d-032c-4d81-aa76-dece5703b643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please restart kernel\n",
    "import os\n",
    "os.environ[\"BIONEMO_HOME\"]=\"/workspace/bionemo\"\n",
    "os.environ[\"WANDB_ENTITY\"]=\"\"\n",
    "os.environ[\"WANDB_PROJECT\"]=\"BioNeMo_protein_LLM_finetuning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd725a-6acc-4867-9052-a29c7886ec17",
   "metadata": {},
   "source": [
    "The BioNeMo Framework offers sample code for three downstream tasks. The first task is to predict the 10 subcellular localizations of a protein, the second is to predict the melting temperature of a protein, and the third is to predict the 3-state structure of a protein. This article will focus on the third task as an example. Specifically, it involves predicting whether each amino acid in a sequence belongs to a helix, sheet, or coil.\n",
    "\n",
    "The sample data for this task can be found in the folder below.\n",
    "\n",
    "/workspace/bionemo/examples/tests/test_data/protein/downstream/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119634af-3177-4631-82e6-3e48dc4c0186",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -q -O /tmp/ngccli_linux.zip --content-disposition https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/3.38.0/files/ngccli_linux.zip && unzip -o /tmp/ngccli_linux.zip -d /tmp && chmod u+x /tmp/ngc-cli/ngc && rm /tmp/ngccli_linux.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7c5f6-e43c-4bfa-95a8-c3aa69ecd5cd",
   "metadata": {},
   "source": [
    "Next, to download a pretrained model, you will need to install NGC (NVIDIA GPU Cloud) and configure your NGC settings. Here’s how you can proceed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7d63b-649c-4d00-ba01-be0e494bcb13",
   "metadata": {
    "tags": []
   },
   "source": [
    "/tmp/ngc-cli/ngc config set\n",
    "\n",
    "To complete the configuration of your NGC CLI, follow these steps, inputting the requested details:\n",
    "\n",
    "`API Key`: Enter your API key from the NVIDIA GPU Cloud (NGC). This key is critical for authentication and accessing the services.\n",
    "\n",
    "`CLI Output Format`: You can choose the desired output format for your CLI responses. If unsure, the default format is usually fine, so just press \"Enter\".\n",
    "\n",
    "`Organization (Org)`: Choose an organization other than 'no-org'. If you are part of an NVIDIA-approved organization, input its name; otherwise, consult NGC documentation or your NGC account settings for possible values.\n",
    "\n",
    "`Team`: If your NGC usage involves a specific team setup, enter the team name. If not, just press \"Enter\" to skip this step.\n",
    "\n",
    "`ACE`: This is typically for advanced configuration settings related to application, compute, and environment. Press \"Enter\" to accept default settings unless specific modifications are required.\n",
    "\n",
    "After configuring NGC, you can download the model using the command below. Replace <model_name> with the actual name of the model you wish to download, for example, ESM-2nv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3e809-f04d-477d-9c28-9348bb55f670",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python download_models.py --download_dir /workspace/bionemo/models esm2nv_650m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363e3ea-d76a-4822-a370-1d7d7e29dce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model saving\n",
    "import wandb\n",
    "with wandb.init(name=\"model_upload\") as run:\n",
    "    artifact = wandb.Artifact(\n",
    "        name=\"esm2nv_650m\",\n",
    "        type=\"model\",\n",
    "        description=\"original esm2nv_650m\",\n",
    "        metadata={\"path\":\"bionemo/models/esm2nv_650M_converted.nemo\"},\n",
    "    )\n",
    "    artifact.add_file(\"/workspace/bionemo/models/esm2nv_650M_converted.nemo\")\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc8482-7fca-480b-8c41-f3524af136d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset saving\n",
    "with wandb.init(name=\"data_upload\") as run:\n",
    "    artifact = wandb.Artifact(\n",
    "        name=\"downstream_taskdataset\",\n",
    "        type=\"dataset\",\n",
    "        description=\"bionemo/examples/tests/test_data/protein/downstream\",\n",
    "        metadata={\"path\":\"/workspace/bionemo/examples/tests/test_data/protein/downstream\"},\n",
    "    )\n",
    "    artifact.add_dir(\"/workspace/bionemo/examples/tests/test_data/protein/downstream\")\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57672e-c394-419a-8b8b-a34950727c4c",
   "metadata": {},
   "source": [
    "The downloaded model is a .nemo file and is saved in /workspace/bionemo/models. Then, open the following YAML file:\n",
    "\n",
    "/workspace/bionemo/examples/protein/esm2nv/conf/downstream_flip_sec_LORA.yaml\n",
    "\n",
    "Set the following parameters as needed:\n",
    "\n",
    "* `restore_from_path`: Set to the path of the .nemo file of the pretrained model checkpoint.\n",
    "* `trainer.devices`, `trainer.num_nodes`: Set to the number of GPUs and nodes to use.\n",
    "* `trainer.max_epochs`: Set to the number of epochs you want to train.\n",
    "* `trainer.val_check_interval`: Set to the number of steps at which to perform validation.\n",
    "* `model.micro_batch_size`: Set to the microbatch size for training.\n",
    "* `data.task_name`: Set to any name you choose.\n",
    "* `data.task_type`: Current options include token-level-classification, classification (sequence level), and regression (sequence level). Set preprocessed_data_path to the path of the parent folder of dataset_path.\n",
    "* `wandb_artifacts.wandb_use_artifact_data_path`: This is the path of the wandb artifacts for the data being used.\n",
    "* `wandb_artifacts.wandb_use_artifact_model_path`: This is the path of the wandb artifacts for the model being used.\n",
    "* `dataset_path`: Set to the path of the folder that includes train/val/test folders. For example, set it to the path/to/data mentioned above.\n",
    "* `dataset.train`, `dataset.val`, `dataset.test`: Set to CSV name or range.\n",
    "* `sequence_column`: Set to the name of the column containing the sequence. Example: sequence\n",
    "* `target_column`: Set to the name of the column containing the target. Example: scl_label\n",
    "* `target_size`: The number of classes for each label in classification.\n",
    "* `num_classes`: Set to target_size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e089ad-5d15-42c2-ac9e-5586d6195e26",
   "metadata": {},
   "source": [
    "Then, you execute the fine-tuning using the following command. BioNeMo Framework v1.4 has introduced a new feature, a fine-tuning method called LoRa. LoRa is an efficient fine-tuning method that, instead of fine-tuning all weights of a pretrained large-scale language model, fine-tunes two smaller matrices that approximate the large weight matrix. This approach allows for more efficient parameter updates while preserving the performance of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf025624-8c35-4fc3-b622-961b1fa1b786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3928e2bb-2c77-43d3-849e-9d6084d96b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# If the directory exists and contains checkpoint data, you should delete it. This is a precautionary measure to ensure that running the following code does not encounter issues with existing data. \n",
    "checkpoint_dir = \"/workspace/bionemo/results/nemo_experiments/esm2nv_flip/esm2nv_flip_secondary_structure_finetuning_encoder_frozen_False/checkpoints\"\n",
    "if os.path.exists(checkpoint_dir) and os.listdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49488d3f-147a-4639-8548-9421abcf0881",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python examples/protein/downstream/downstream_flip.py\\\n",
    " --config-path=\"../esm2nv/conf\"\\\n",
    " --config-name=downstream_sec_str_LORA\\\n",
    " ++trainer.devices=1\\\n",
    " ++trainer.num_nodes=1\\\n",
    " ++trainer.max_epochs=100\\\n",
    " ++wandb_artifacts.wandb_use_artifact_data_path='${oc.env:WANDB_ENTITY}/${oc.env:WANDB_PROJECT}/downstream_taskdataset:v0'\\\n",
    " ++wandb_artifacts.wandb_use_artifact_model_path='${oc.env:WANDB_ENTITY}/${oc.env:WANDB_PROJECT}/esm2nv_650m:v0'\\\n",
    " ++model.data.dataset_path=/\\\n",
    " ++model.restore_encoder_path=/esm2nv_650M_converted.nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a5657-a458-41e5-a31b-8b55af7471b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504e28f4-36b0-4eca-bc85-9a3e76334587",
   "metadata": {},
   "source": [
    "Please open the following file\n",
    "\n",
    "/workspace/bionemo/examples/protein/esm2nv/conf/infer.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db06b9c0-f089-4a61-a950-d220f936eef3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Please update the following information. You can use a pretrained model provided, or you can use a model that you have trained yourself. Make sure to specify which option you choose and adjust the settings accordingly, such as the path to the model checkpoint or any specific configurations needed for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7c61d-4af1-437d-bf3b-59fc85581e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "downstream_task:\n",
    " restore_from_path: \"${oc.env:BIONEMO_HOME}/models/protein/esm2nv/esm2nv_650M_converted.nemo\" # 事前学習済みモデルのパス"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90071908-294c-4b25-8a04-de9d60817373",
   "metadata": {},
   "source": [
    "Please open the following Jupyter Notebook:\n",
    "\n",
    "/workspace/bionemo/examples/protein/esm2nv/nbs/inference_interactive.ipynb\n",
    "\n",
    "Then, proceed to execute the cells in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874d608-08e0-4b90-9eae-bfdc25e8384b",
   "metadata": {},
   "source": [
    "# Use your own data (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf106d-5013-4bcb-93e8-abc413f51aae",
   "metadata": {},
   "source": [
    "Please prepare directories named train, val, and test in the local directory /home/bionemo/. In each folder, prepare files named x000.csv, x001.csv, and so on.\n",
    "\n",
    "## Pre-training Data Preparation\n",
    "For the format of the pre-training data, please refer to the following file:\n",
    "\n",
    "/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200/uf50/train/x000.csv\n",
    "\n",
    "The columns record_id and sequence are mandatory, and other columns are optional. You will also need to modify the data section of the following YAML file to change the number of data entries and paths:\n",
    "\n",
    "/workspace/bionemo/examples/protein/esm2nv/conf/base_config.yaml\n",
    "\n",
    "## Fine-Tuning Data Preparation\n",
    "For the format of the fine-tuning data, please refer to the following file:\n",
    "\n",
    "/workspace/bionemo/examples/tests/test_data/protein/downstream/train/x000.csv\n",
    "\n",
    "The columns id, sequence, and target (either 3-state or 8-state) are mandatory, and other columns are optional. Additionally, you will need to change the number of data entries, paths, and target information in the data section of the following YAML file:\n",
    "\n",
    "/workspace/bionemo/examples/protein/esm2nv/conf/downstream_sec_str_LORA.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb634d7-79a9-419e-bd86-b786205ddb75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
